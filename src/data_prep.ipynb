{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating RP images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid sensor data for Subject 8, Activity 11, Trial 2\n",
      "No valid sensor data for Subject 8, Activity 11, Trial 3\n",
      "Recurrence plots for all trials have been created and saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyts.image import RecurrencePlot\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Define paths\n",
    "dataset_paths = {\n",
    "    \"trial1\": \"D:\\\\UP-FALL\",\n",
    "    \"trial2\": \"D:\\\\UP-FALL\\\\trial2\",\n",
    "    \"trial3\": \"D:\\\\UP-FALL\\\\trial3\"\n",
    "}\n",
    "output_base_path = \"D:\\\\UP-FALL\\\\v2\"\n",
    "os.makedirs(output_base_path, exist_ok=True)\n",
    "\n",
    "# Define column names\n",
    "columns = [\n",
    "    \"Timestamp\",\n",
    "    \"AnkleAccelerometer_x\", \"AnkleAccelerometer_y\", \"AnkleAccelerometer_z\",\n",
    "    \"AnkleAngularVelocity_x\", \"AnkleAngularVelocity_y\", \"AnkleAngularVelocity_z\",\n",
    "    \"AnkleLuminosity\",\n",
    "    # Other sensors are ignored for this task\n",
    "    \"RightPocketAccelerometer_x\", \"RightPocketAccelerometer_y\", \"RightPocketAccelerometer_z\",\n",
    "    \"RightPocketAngularVelocity_x\", \"RightPocketAngularVelocity_y\", \"RightPocketAngularVelocity_z\",\n",
    "    \"RightPocketLuminosity\",\n",
    "    \"BeltAccelerometer_x\", \"BeltAccelerometer_y\", \"BeltAccelerometer_z\",\n",
    "    \"BeltAngularVelocity_x\", \"BeltAngularVelocity_y\", \"BeltAngularVelocity_z\",\n",
    "    \"BeltLuminosity\",\n",
    "    \"NeckAccelerometer_x\", \"NeckAccelerometer_y\", \"NeckAccelerometer_z\",\n",
    "    \"NeckAngularVelocity_x\", \"NeckAngularVelocity_y\", \"NeckAngularVelocity_z\",\n",
    "    \"NeckLuminosity\",\n",
    "    \"WristAccelerometer_x\", \"WristAccelerometer_y\", \"WristAccelerometer_z\",\n",
    "    \"WristAngularVelocity_x\", \"WristAngularVelocity_y\", \"WristAngularVelocity_z\",\n",
    "    \"WristLuminosity\",\n",
    "    \"BrainSensor\",\n",
    "    \"Infrared1\", \"Infrared2\", \"Infrared3\", \"Infrared4\", \"Infrared5\", \"Infrared6\",\n",
    "    \"Subject\", \"Activity\", \"Trial\"\n",
    "]\n",
    "\n",
    "# Define function to generate recurrence plot images\n",
    "# Define function to generate recurrence plot images\n",
    "def generate_recurrence_plot(data, output_path):\n",
    "    try:\n",
    "        # Create RecurrencePlot object without the image_size parameter\n",
    "        rp = RecurrencePlot(dimension=1, threshold='point', percentage=10)\n",
    "        rp_image = rp.fit_transform(data.reshape(1, -1))[0]\n",
    "        np.save(output_path, rp_image)  # Save as .npy\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating recurrence plot for {output_path}: {e}\")\n",
    "\n",
    "\n",
    "# Function to process sensor data\n",
    "def process_sensor_data(trial, subject, activity, trial_path, output_path, window_size, overlap):\n",
    "    csv_file = os.path.join(trial_path, f\"Subject{subject}Activity{activity}Trial{trial}.csv\")  # Corrected filename format\n",
    "    \n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(csv_file):\n",
    "        print(f\"File not found: {csv_file}\")\n",
    "        return\n",
    "\n",
    "    # Load sensor data\n",
    "    try:\n",
    "        data = pd.read_csv(csv_file, skiprows=2, names=columns, usecols=range(46))\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {csv_file}: {e}\")\n",
    "        return\n",
    "\n",
    "    # Compute magnitudes for accelerometer and angular velocity\n",
    "    acc_magnitude = np.sqrt(data[\"AnkleAccelerometer_x\"]**2 + \n",
    "                            data[\"AnkleAccelerometer_y\"]**2 + \n",
    "                            data[\"AnkleAccelerometer_z\"]**2)\n",
    "    gyro_magnitude = np.sqrt(data[\"AnkleAngularVelocity_x\"]**2 + \n",
    "                             data[\"AnkleAngularVelocity_y\"]**2 + \n",
    "                             data[\"AnkleAngularVelocity_z\"]**2)\n",
    "\n",
    "    # **Handle Empty Data**\n",
    "    if acc_magnitude.empty or gyro_magnitude.empty:\n",
    "        print(f\"No valid sensor data for Subject {subject}, Activity {activity}, Trial {trial}\")\n",
    "        return\n",
    "\n",
    "    # Normalize magnitudes\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    acc_magnitude = scaler.fit_transform(acc_magnitude.values.reshape(-1, 1)).flatten()\n",
    "    gyro_magnitude = scaler.fit_transform(gyro_magnitude.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Determine window length in rows\n",
    "    sampling_rate = 20  # Assuming 20 Hz\n",
    "    window_rows = int(window_size * sampling_rate)\n",
    "    overlap_rows = int(overlap * window_rows)\n",
    "\n",
    "    # Create output directory\n",
    "    activity_output_path = os.path.join(output_path, f\"Subject{subject}_Activity{activity}_Trial{trial}\")\n",
    "    os.makedirs(activity_output_path, exist_ok=True)\n",
    "\n",
    "    # Generate recurrence plots for each window\n",
    "    start = 0\n",
    "    window_idx = 0\n",
    "    while start + window_rows <= len(acc_magnitude):\n",
    "        acc_window = acc_magnitude[start:start + window_rows]\n",
    "        gyro_window = gyro_magnitude[start:start + window_rows]\n",
    "\n",
    "        if len(acc_window) == 0 or len(gyro_window) == 0:\n",
    "            print(f\"Skipping empty window for Subject {subject}, Activity {activity}, Trial {trial}\")\n",
    "            start += overlap_rows\n",
    "            window_idx += 1\n",
    "            continue\n",
    "\n",
    "        # Generate and save recurrence plots\n",
    "        acc_output_file = os.path.join(activity_output_path, f\"window{window_idx}_acc.npy\")\n",
    "        gyro_output_file = os.path.join(activity_output_path, f\"window{window_idx}_gyro.npy\")\n",
    "        try:\n",
    "            generate_recurrence_plot(acc_window, acc_output_file)\n",
    "            generate_recurrence_plot(gyro_window, gyro_output_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating recurrence plot for {acc_output_file} or {gyro_output_file}: {e}\")\n",
    "\n",
    "        # Move the window forward\n",
    "        start += overlap_rows\n",
    "        window_idx += 1\n",
    "\n",
    "\n",
    "# Process all trials\n",
    "for trial_name, trial_path in dataset_paths.items():\n",
    "    output_path = os.path.join(output_base_path, f\"{trial_name}_sensor_rp_images\")\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    for subject in range(1, 18):  # Subjects 1 to 17\n",
    "        for activity in range(1, 12):  # Activities 1 to 11\n",
    "            # Set windowing parameters based on activity duration\n",
    "            if activity in [1, 2, 3, 4, 5, 9]:  # Short activities (10s)\n",
    "                window_size = 3  # seconds\n",
    "                overlap = 0.5\n",
    "            else:  # Long activities (30-60s)\n",
    "                window_size = 5  # seconds\n",
    "                overlap = 0.5\n",
    "\n",
    "            process_sensor_data(trial_name[-1], subject, activity, trial_path, output_path, window_size, overlap)\n",
    "\n",
    "print(\"Recurrence plots for all trials have been created and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum image size: (60, 60)\n",
      "Maximum image size: (100, 100)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define the dataset paths\n",
    "dataset_paths = [\n",
    "    \"D:\\\\UP-FALL\\\\v2\\\\trial1_sensor_rp_images\",\n",
    "    \"D:\\\\UP-FALL\\\\v2\\\\trial2_sensor_rp_images\",\n",
    "    \"D:\\\\UP-FALL\\\\v2\\\\trial3_sensor_rp_images\"\n",
    "]\n",
    "\n",
    "# Initialize variables to store min and max sizes\n",
    "min_size = None\n",
    "max_size = None\n",
    "\n",
    "# Function to update min and max size\n",
    "def update_min_max(size, min_size, max_size):\n",
    "    if min_size is None or size < min_size:\n",
    "        min_size = size\n",
    "    if max_size is None or size > max_size:\n",
    "        max_size = size\n",
    "    return min_size, max_size\n",
    "\n",
    "# Loop through all dataset paths\n",
    "for dataset_path in dataset_paths:\n",
    "    for root, _, files in os.walk(dataset_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.npy'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    # Load the image\n",
    "                    image = np.load(file_path)\n",
    "                    # Get the size (height, width)\n",
    "                    size = image.shape[:2]\n",
    "                    # Update min and max sizes\n",
    "                    min_size, max_size = update_min_max(size, min_size, max_size)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Print the results\n",
    "print(\"Minimum image size:\", min_size)\n",
    "print(\"Maximum image size:\", max_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing of RP images is complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Paths\n",
    "rp_image_paths = [\n",
    "    \"D:\\\\UP-FALL\\\\v2\\\\trial1_sensor_rp_images\",\n",
    "    \"D:\\\\UP-FALL\\\\v2\\\\trial2_sensor_rp_images\",\n",
    "    \"D:\\\\UP-FALL\\\\v2\\\\trial3_sensor_rp_images\"\n",
    "]\n",
    "\n",
    "output_paths = [\n",
    "    \"D:\\\\UP-FALL\\\\v2\\\\trial1_sensor_rp_images_resized\",\n",
    "    \"D:\\\\UP-FALL\\\\v2\\\\trial2_sensor_rp_images_resized\",\n",
    "    \"D:\\\\UP-FALL\\\\v2\\\\trial3_sensor_rp_images_resized\"\n",
    "]\n",
    "\n",
    "target_size = (32, 32)  # Target size for resizing\n",
    "\n",
    "# Ensure output directories exist\n",
    "for path in output_paths:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# Function to resize images\n",
    "def resize_rp_images(input_path, output_path):\n",
    "    for root, _, files in os.walk(input_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.npy'):\n",
    "                input_file = os.path.join(root, file)\n",
    "                # Preserve the relative path in the output directory\n",
    "                relative_path = os.path.relpath(root, input_path)\n",
    "                output_dir = os.path.join(output_path, relative_path)\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "                output_file = os.path.join(output_dir, file)\n",
    "\n",
    "                try:\n",
    "                    # Load the .npy file\n",
    "                    image = np.load(input_file)\n",
    "                    # Convert to PIL image and resize\n",
    "                    resized_image = Image.fromarray(image).resize(target_size, Image.Resampling.LANCZOS)\n",
    "                    # Save the resized image back as .npy\n",
    "                    np.save(output_file, np.array(resized_image))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error resizing {input_file}: {e}\")\n",
    "\n",
    "# Resize RP images for each trial\n",
    "for input_path, output_path in zip(rp_image_paths, output_paths):\n",
    "    resize_rp_images(input_path, output_path)\n",
    "\n",
    "print(\"Resizing of RP images is complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video frames processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing video data for Subject 8, Activity 11, Trial 2\n",
      "Missing video data for Subject 8, Activity 11, Trial 3\n",
      "Summed difference images for all trials have been created and saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# Paths\n",
    "trial_paths = {\n",
    "    \"trial1\": \"D:\\\\UP-FALL\",\n",
    "    \"trial2\": \"D:\\\\UP-FALL\\\\trial2\",\n",
    "    \"trial3\": \"D:\\\\UP-FALL\\\\trial3\"\n",
    "}\n",
    "\n",
    "output_base_path = \"D:\\\\UP-FALL\\\\v2\"\n",
    "os.makedirs(output_base_path, exist_ok=True)\n",
    "\n",
    "# Target size for resizing\n",
    "target_size = (32, 32)\n",
    "\n",
    "# Function to extract and process frames for summed differences\n",
    "def process_video_summed_diff(trial, subject, activity, trial_path, output_path, window_size, overlap):\n",
    "    zip_file = os.path.join(trial_path, f\"Subject{subject}Activity{activity}Trial{trial}Camera1.zip\")\n",
    "    \n",
    "    # Check if the ZIP file exists\n",
    "    if not os.path.exists(zip_file):\n",
    "        print(f\"Missing video data for Subject {subject}, Activity {activity}, Trial {trial}\")\n",
    "        return\n",
    "    \n",
    "    # Temporary extraction path\n",
    "    temp_extract_path = os.path.join(trial_path, \"temp_frames\")\n",
    "    os.makedirs(temp_extract_path, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        # Extract frames\n",
    "        with ZipFile(zip_file, 'r') as zip_ref:\n",
    "            zip_ref.extractall(temp_extract_path)\n",
    "\n",
    "        # Get all image files in sorted order\n",
    "        frame_files = sorted([os.path.join(temp_extract_path, f) for f in os.listdir(temp_extract_path) if f.endswith('.jpg') or f.endswith('.png')])\n",
    "\n",
    "        if len(frame_files) < 2:\n",
    "            print(f\"Not enough frames for Subject {subject}, Activity {activity}, Trial {trial}\")\n",
    "            return\n",
    "\n",
    "        # Determine window length and overlap in terms of frames\n",
    "        sampling_rate = 20  # Assuming 20 frames per second\n",
    "        window_frames = int(window_size * sampling_rate)\n",
    "        overlap_frames = int(overlap * window_frames)\n",
    "\n",
    "        # Create output directory\n",
    "        activity_output_path = os.path.join(output_path, f\"Subject{subject}_Activity{activity}_Trial{trial}\")\n",
    "        os.makedirs(activity_output_path, exist_ok=True)\n",
    "\n",
    "        # Process frames in windows\n",
    "        start = 0\n",
    "        window_idx = 0\n",
    "        while start + window_frames <= len(frame_files):\n",
    "            summed_diff = None\n",
    "\n",
    "            # Compute summed differences for frames in the window\n",
    "            for i in range(start, start + window_frames - 1):\n",
    "                frame1 = cv2.imread(frame_files[i], cv2.IMREAD_GRAYSCALE)\n",
    "                frame2 = cv2.imread(frame_files[i + 1], cv2.IMREAD_GRAYSCALE)\n",
    "                if frame1 is not None and frame2 is not None:\n",
    "                    diff = cv2.absdiff(frame2, frame1)\n",
    "                    if summed_diff is None:\n",
    "                        summed_diff = np.zeros_like(diff, dtype=np.float32)\n",
    "                    summed_diff += diff.astype(np.float32)\n",
    "\n",
    "            if summed_diff is not None:\n",
    "                # Normalize and resize the summed difference image\n",
    "                summed_diff = cv2.normalize(summed_diff, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "                resized_summed_diff = cv2.resize(summed_diff, target_size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "                # Save the resulting image\n",
    "                output_file = os.path.join(activity_output_path, f\"window{window_idx}_summed_diff.jpg\")\n",
    "                cv2.imwrite(output_file, resized_summed_diff)\n",
    "\n",
    "            # Move the window forward\n",
    "            start += overlap_frames\n",
    "            window_idx += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing video for Subject {subject}, Activity {activity}, Trial {trial}: {e}\")\n",
    "    finally:\n",
    "        # Clean up temporary files\n",
    "        for f in os.listdir(temp_extract_path):\n",
    "            os.remove(os.path.join(temp_extract_path, f))\n",
    "        os.rmdir(temp_extract_path)\n",
    "\n",
    "# Process all trials\n",
    "for trial_name, trial_path in trial_paths.items():\n",
    "    output_path = os.path.join(output_base_path, f\"{trial_name}_summed_diff_images\")\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    for subject in range(1, 18):  # Subjects 1 to 17\n",
    "        for activity in range(1, 12):  # Activities 1 to 11\n",
    "            # Set windowing parameters based on activity duration\n",
    "            if activity in [1, 2, 3, 4, 5, 9]:  # Short activities (10s)\n",
    "                window_size = 3  # seconds\n",
    "                overlap = 0.5\n",
    "            else:  # Long activities (30-60s)\n",
    "                window_size = 5  # seconds\n",
    "                overlap = 0.5\n",
    "\n",
    "            process_video_summed_diff(trial_name[-1], subject, activity, trial_path, output_path, window_size, overlap)\n",
    "\n",
    "print(\"Summed difference images for all trials have been created and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating GAF images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid sensor data for Subject 8, Activity 11, Trial 2\n",
      "No valid sensor data for Subject 8, Activity 11, Trial 3\n",
      "GAF images have been created.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyts.image import GramianAngularField\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Paths\n",
    "dataset_paths = {\n",
    "    \"trial1\": \"D:\\\\UP-FALL\",\n",
    "    \"trial2\": \"D:\\\\UP-FALL\\\\trial2\",\n",
    "    \"trial3\": \"D:\\\\UP-FALL\\\\trial3\"\n",
    "}\n",
    "output_base_path = \"D:\\\\UP-FALL\\\\v2\"\n",
    "os.makedirs(output_base_path, exist_ok=True)\n",
    "\n",
    "# Columns\n",
    "columns = [\n",
    "    \"Timestamp\",\n",
    "    \"AnkleAccelerometer_x\", \"AnkleAccelerometer_y\", \"AnkleAccelerometer_z\",\n",
    "    \"AnkleAngularVelocity_x\", \"AnkleAngularVelocity_y\", \"AnkleAngularVelocity_z\",\n",
    "    # Other columns skipped for simplicity\n",
    "]\n",
    "\n",
    "# Generate GAF\n",
    "def generate_gaf(data, output_path, image_size=32):\n",
    "    try:\n",
    "        gaf = GramianAngularField(image_size=image_size, method='summation')\n",
    "        gaf_image = gaf.fit_transform(data.reshape(1, -1))[0]\n",
    "        np.save(output_path, gaf_image)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating GAF for {output_path}: {e}\")\n",
    "\n",
    "# Process Sensor Data\n",
    "def process_sensor_data(trial, subject, activity, trial_path, output_path, window_size, overlap):\n",
    "    csv_file = os.path.join(trial_path, f\"Subject{subject}Activity{activity}Trial{trial}.csv\")\n",
    "    \n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(csv_file):\n",
    "        print(f\"File not found: {csv_file}\")\n",
    "        return\n",
    "\n",
    "    # Load sensor data\n",
    "    try:\n",
    "        data = pd.read_csv(csv_file, skiprows=2, names=columns, usecols=range(7))  # Adjust column indices as needed\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {csv_file}: {e}\")\n",
    "        return\n",
    "\n",
    "    # Compute magnitudes for accelerometer and angular velocity\n",
    "    acc_magnitude = np.sqrt(data[\"AnkleAccelerometer_x\"]**2 +\n",
    "                            data[\"AnkleAccelerometer_y\"]**2 +\n",
    "                            data[\"AnkleAccelerometer_z\"]**2)\n",
    "    gyro_magnitude = np.sqrt(data[\"AnkleAngularVelocity_x\"]**2 +\n",
    "                             data[\"AnkleAngularVelocity_y\"]**2 +\n",
    "                             data[\"AnkleAngularVelocity_z\"]**2)\n",
    "\n",
    "    # **Handle Empty Data**\n",
    "    if acc_magnitude.empty or gyro_magnitude.empty:\n",
    "        print(f\"No valid sensor data for Subject {subject}, Activity {activity}, Trial {trial}\")\n",
    "        return\n",
    "\n",
    "    # Normalize magnitudes\n",
    "    try:\n",
    "        scaler = MinMaxScaler((0, 1))\n",
    "        acc_magnitude = scaler.fit_transform(acc_magnitude.values.reshape(-1, 1)).flatten()\n",
    "        gyro_magnitude = scaler.fit_transform(gyro_magnitude.values.reshape(-1, 1)).flatten()\n",
    "    except ValueError as e:\n",
    "        print(f\"Error normalizing data for Subject {subject}, Activity {activity}, Trial {trial}: {e}\")\n",
    "        return\n",
    "\n",
    "    # Determine window length in rows\n",
    "    sampling_rate = 20\n",
    "    window_rows = int(window_size * sampling_rate)\n",
    "    overlap_rows = int(overlap * window_rows)\n",
    "\n",
    "    # Create output directory\n",
    "    activity_output_path = os.path.join(output_path, f\"Subject{subject}_Activity{activity}_Trial{trial}\")\n",
    "    os.makedirs(activity_output_path, exist_ok=True)\n",
    "\n",
    "    # Generate GAF for each window\n",
    "    start = 0\n",
    "    window_idx = 0\n",
    "    while start + window_rows <= len(acc_magnitude):\n",
    "        acc_window = acc_magnitude[start:start + window_rows]\n",
    "        gyro_window = gyro_magnitude[start:start + window_rows]\n",
    "\n",
    "        # Check for empty windows\n",
    "        if len(acc_window) == 0 or len(gyro_window) == 0:\n",
    "            print(f\"Skipping empty window for Subject {subject}, Activity {activity}, Trial {trial}\")\n",
    "            start += overlap_rows\n",
    "            window_idx += 1\n",
    "            continue\n",
    "\n",
    "        acc_output_file = os.path.join(activity_output_path, f\"window{window_idx}_acc.npy\")\n",
    "        gyro_output_file = os.path.join(activity_output_path, f\"window{window_idx}_gyro.npy\")\n",
    "        generate_gaf(acc_window, acc_output_file)\n",
    "        generate_gaf(gyro_window, gyro_output_file)\n",
    "\n",
    "        # Move window forward\n",
    "        start += overlap_rows\n",
    "        window_idx += 1\n",
    "\n",
    "\n",
    "# Generate GAF for all trials\n",
    "for trial_name, trial_path in dataset_paths.items():\n",
    "    output_path = os.path.join(output_base_path, f\"{trial_name}_sensor_gaf_images\")\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    for subject in range(1, 18):\n",
    "        for activity in range(1, 12):\n",
    "            if activity in [1, 2, 3, 4, 5, 9]:\n",
    "                process_sensor_data(trial_name[-1], subject, activity, trial_path, output_path, 3, 0.5)\n",
    "            else:\n",
    "                process_sensor_data(trial_name[-1], subject, activity, trial_path, output_path, 5, 0.5)\n",
    "\n",
    "print(\"GAF images have been created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusion of GAF and Summed Difference Images is complete.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "gaf_paths = {\n",
    "    \"trial1\": \"D:\\\\UP-FALL\\\\v2\\\\trial1_sensor_gaf_images\",\n",
    "    \"trial2\": \"D:\\\\UP-FALL\\\\v2\\\\trial2_sensor_gaf_images\",\n",
    "    \"trial3\": \"D:\\\\UP-FALL\\\\v2\\\\trial3_sensor_gaf_images\"\n",
    "}\n",
    "summed_diff_paths = {\n",
    "    \"trial1\": \"D:\\\\UP-FALL\\\\v2\\\\trial1_summed_diff_images\",\n",
    "    \"trial2\": \"D:\\\\UP-FALL\\\\v2\\\\trial2_summed_diff_images\",\n",
    "    \"trial3\": \"D:\\\\UP-FALL\\\\v2\\\\trial3_summed_diff_images\"\n",
    "}\n",
    "fused_output_path = \"D:\\\\UP-FALL\\\\v2\\\\fused_gaf_sd\"\n",
    "os.makedirs(fused_output_path, exist_ok=True)\n",
    "\n",
    "# Fuse GAF with summed difference images\n",
    "def fuse_images(gaf_path, sd_path, trial_output_path):\n",
    "    for root, _, files in os.walk(sd_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.jpg'):\n",
    "                sd_image_path = os.path.join(root, file)\n",
    "                gaf_folder = os.path.relpath(root, sd_path)\n",
    "                gaf_folder_path = os.path.join(gaf_path, gaf_folder)\n",
    "\n",
    "                # Check if corresponding GAF files exist\n",
    "                gaf_acc_path = os.path.join(gaf_folder_path, file.replace(\"summed_diff.jpg\", \"acc.npy\"))\n",
    "                gaf_gyro_path = os.path.join(gaf_folder_path, file.replace(\"summed_diff.jpg\", \"gyro.npy\"))\n",
    "\n",
    "                if not os.path.exists(gaf_acc_path) or not os.path.exists(gaf_gyro_path):\n",
    "                    print(f\"Missing GAF files for {file}\")\n",
    "                    continue\n",
    "\n",
    "                # Load GAF and SD images\n",
    "                sd_image = cv2.imread(sd_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "                gaf_acc = np.load(gaf_acc_path)\n",
    "                gaf_gyro = np.load(gaf_gyro_path)\n",
    "\n",
    "                # Stack as channels\n",
    "                fused_image = np.stack([sd_image, gaf_acc, gaf_gyro], axis=-1)\n",
    "\n",
    "                # Save fused image\n",
    "                output_dir = os.path.join(trial_output_path, gaf_folder)\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "                output_file = os.path.join(output_dir, file.replace(\"summed_diff.jpg\", \"fused.npy\"))\n",
    "                np.save(output_file, fused_image)\n",
    "\n",
    "# Fuse for each trial\n",
    "for trial_name in [\"trial1\", \"trial2\", \"trial3\"]:\n",
    "    fuse_images(gaf_paths[trial_name], summed_diff_paths[trial_name], os.path.join(fused_output_path, f\"{trial_name}_fused_images\"))\n",
    "\n",
    "print(\"Fusion of GAF and Summed Difference Images is complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding additional sensor data (wrist acceleration and angular velocity magnitudes converted into RP images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid sensor data for Subject 8, Activity 11, Trial 2\n",
      "No valid sensor data for Subject 8, Activity 11, Trial 3\n",
      "RP images for wrist sensors created.\n"
     ]
    }
   ],
   "source": [
    "# Function to process wrist sensor data only\n",
    "def process_wrist_sensor_data(trial, subject, activity, trial_path, output_path, window_size, overlap):\n",
    "    csv_file = os.path.join(trial_path, f\"Subject{subject}Activity{activity}Trial{trial}.csv\")\n",
    "    if not os.path.exists(csv_file):\n",
    "        print(f\"File not found: {csv_file}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Load CSV with explicitly defined columns and skip unnecessary rows\n",
    "        data = pd.read_csv(csv_file, skiprows=2, names=columns, usecols=range(46))\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {csv_file}: {e}\")\n",
    "        return\n",
    "\n",
    "    # Compute magnitudes for wrist sensors\n",
    "    acc_magnitude = np.sqrt(data[\"WristAccelerometer_x\"]**2 + \n",
    "                            data[\"WristAccelerometer_y\"]**2 + \n",
    "                            data[\"WristAccelerometer_z\"]**2)\n",
    "    gyro_magnitude = np.sqrt(data[\"WristAngularVelocity_x\"]**2 + \n",
    "                             data[\"WristAngularVelocity_y\"]**2 + \n",
    "                             data[\"WristAngularVelocity_z\"]**2)\n",
    "\n",
    "    # **Handle Empty Data**\n",
    "    if acc_magnitude.empty or gyro_magnitude.empty:\n",
    "        print(f\"No valid sensor data for Subject {subject}, Activity {activity}, Trial {trial}\")\n",
    "        return\n",
    "\n",
    "    # Normalize magnitudes\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    try:\n",
    "        acc_magnitude = scaler.fit_transform(acc_magnitude.values.reshape(-1, 1)).flatten()\n",
    "        gyro_magnitude = scaler.fit_transform(gyro_magnitude.values.reshape(-1, 1)).flatten()\n",
    "    except ValueError as e:\n",
    "        print(f\"Skipping due to empty or invalid data for Subject {subject}, Activity {activity}, Trial {trial}: {e}\")\n",
    "        return\n",
    "\n",
    "    # Determine window size and overlap\n",
    "    sampling_rate = 20\n",
    "    window_rows = int(window_size * sampling_rate)\n",
    "    overlap_rows = int(overlap * window_rows)\n",
    "\n",
    "    # Create output directory\n",
    "    activity_output_path = os.path.join(output_path, f\"Subject{subject}_Activity{activity}_Trial{trial}\")\n",
    "    os.makedirs(activity_output_path, exist_ok=True)\n",
    "\n",
    "    # Generate RP images for each window\n",
    "    start = 0\n",
    "    window_idx = 0\n",
    "    while start + window_rows <= len(acc_magnitude):\n",
    "        acc_window = acc_magnitude[start:start + window_rows]\n",
    "        gyro_window = gyro_magnitude[start:start + window_rows]\n",
    "\n",
    "        if len(acc_window) == 0 or len(gyro_window) == 0:\n",
    "            start += overlap_rows\n",
    "            window_idx += 1\n",
    "            continue\n",
    "\n",
    "        acc_output_file = os.path.join(activity_output_path, f\"window{window_idx}_wrist_acc.npy\")\n",
    "        gyro_output_file = os.path.join(activity_output_path, f\"window{window_idx}_wrist_gyro.npy\")\n",
    "        try:\n",
    "            generate_recurrence_plot(acc_window, acc_output_file)\n",
    "            generate_recurrence_plot(gyro_window, gyro_output_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating RP for wrist data in {activity_output_path}: {e}\")\n",
    "\n",
    "        start += overlap_rows\n",
    "        window_idx += 1\n",
    "\n",
    "# Process all trials\n",
    "for trial_name, trial_path in dataset_paths.items():\n",
    "    output_path = os.path.join(output_base_path, f\"{trial_name}_sensor_rp_images_wrist\")\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    for subject in range(1, 18):\n",
    "        for activity in range(1, 12):\n",
    "            if activity in [1, 2, 3, 4, 5, 9]:\n",
    "                window_size = 3\n",
    "                overlap = 0.5\n",
    "            else:\n",
    "                window_size = 5\n",
    "                overlap = 0.5\n",
    "\n",
    "            process_wrist_sensor_data(trial_name[-1], subject, activity, trial_path, output_path, window_size, overlap)\n",
    "\n",
    "print(\"RP images for wrist sensors created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing of RP images for wrist is complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Paths for RP wrist data\n",
    "wrist_rp_paths = [\n",
    "    \"D:\\\\UP-FALL\\\\v2\\\\trial1_sensor_rp_images_wrist\",\n",
    "    \"D:\\\\UP-FALL\\\\v2\\\\trial2_sensor_rp_images_wrist\",\n",
    "    \"D:\\\\UP-FALL\\\\v2\\\\trial3_sensor_rp_images_wrist\"\n",
    "]\n",
    "\n",
    "resized_output_paths = [\n",
    "    \"D:\\\\UP-FALL\\\\v2\\\\trial1_sensor_rp_images_wrist_resized\",\n",
    "    \"D:\\\\UP-FALL\\\\v2\\\\trial2_sensor_rp_images_wrist_resized\",\n",
    "    \"D:\\\\UP-FALL\\\\v2\\\\trial3_sensor_rp_images_wrist_resized\"\n",
    "]\n",
    "\n",
    "target_size = (32, 32)\n",
    "\n",
    "# Ensure output directories exist\n",
    "for path in resized_output_paths:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# Resize function\n",
    "def resize_rp_images(input_path, output_path):\n",
    "    for root, _, files in os.walk(input_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.npy'):\n",
    "                input_file = os.path.join(root, file)\n",
    "                relative_path = os.path.relpath(root, input_path)\n",
    "                output_dir = os.path.join(output_path, relative_path)\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "                output_file = os.path.join(output_dir, file)\n",
    "\n",
    "                try:\n",
    "                    image = np.load(input_file)\n",
    "                    resized_image = Image.fromarray(image).resize(target_size, Image.Resampling.LANCZOS)\n",
    "                    np.save(output_file, np.array(resized_image))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error resizing {input_file}: {e}\")\n",
    "\n",
    "# Resize wrist RP images\n",
    "for input_path, output_path in zip(wrist_rp_paths, resized_output_paths):\n",
    "    resize_rp_images(input_path, output_path)\n",
    "\n",
    "print(\"Resizing of RP images for wrist is complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 0 files during fusion.\n",
      "Skipped 0 files during fusion.\n",
      "Skipped 0 files during fusion.\n",
      "Fusion completed. Combined dataset saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Paths\n",
    "fused_ankle_sd_paths = {\n",
    "    \"trial1\": \"D:\\\\UP-FALL\\\\v2\\\\fused_images_rp_sd_ankle\\\\trial1_fused_images\",\n",
    "    \"trial2\": \"D:\\\\UP-FALL\\\\v2\\\\fused_images_rp_sd_ankle\\\\trial2_fused_images\",\n",
    "    \"trial3\": \"D:\\\\UP-FALL\\\\v2\\\\fused_images_rp_sd_ankle\\\\trial3_fused_images\"\n",
    "}\n",
    "wrist_rp_paths = {\n",
    "    \"trial1\": \"D:\\\\UP-FALL\\\\v2\\\\trial1_sensor_rp_images_wrist_resized\",\n",
    "    \"trial2\": \"D:\\\\UP-FALL\\\\v2\\\\trial2_sensor_rp_images_wrist_resized\",\n",
    "    \"trial3\": \"D:\\\\UP-FALL\\\\v2\\\\trial3_sensor_rp_images_wrist_resized\"\n",
    "}\n",
    "output_base_path = \"D:\\\\UP-FALL\\\\v2\\\\fused_images_rp_sd_ankle_wrist\"\n",
    "os.makedirs(output_base_path, exist_ok=True)\n",
    "\n",
    "# Function to combine wrist RP with the existing fused dataset\n",
    "def combine_wrist_with_fused(fused_dir, wrist_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    skipped = 0\n",
    "\n",
    "    for root, _, files in os.walk(fused_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.npy'):\n",
    "                fused_file_path = os.path.join(root, file)\n",
    "\n",
    "                # Extract relative path for wrist data\n",
    "                relative_path = os.path.relpath(root, fused_dir)\n",
    "                wrist_dir_path = os.path.join(wrist_dir, relative_path)\n",
    "\n",
    "                if not os.path.exists(wrist_dir_path):\n",
    "                    print(f\"Wrist directory not found: {wrist_dir_path}\")\n",
    "                    skipped += 1\n",
    "                    continue\n",
    "\n",
    "                wrist_acc_file = os.path.join(wrist_dir_path, file.replace('fused.npy', 'wrist_acc.npy'))\n",
    "                wrist_gyro_file = os.path.join(wrist_dir_path, file.replace('fused.npy', 'wrist_gyro.npy'))\n",
    "\n",
    "                if not os.path.exists(wrist_acc_file) or not os.path.exists(wrist_gyro_file):\n",
    "                    print(f\"Missing wrist files for {file}: {wrist_acc_file} or {wrist_gyro_file}\")\n",
    "                    skipped += 1\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    # Load data\n",
    "                    fused_data = np.load(fused_file_path)\n",
    "                    wrist_acc = np.load(wrist_acc_file)\n",
    "                    wrist_gyro = np.load(wrist_gyro_file)\n",
    "\n",
    "                    # Ensure wrist data is 2D\n",
    "                    if len(wrist_acc.shape) == 2:\n",
    "                        wrist_acc = wrist_acc[..., np.newaxis]\n",
    "                    if len(wrist_gyro.shape) == 2:\n",
    "                        wrist_gyro = wrist_gyro[..., np.newaxis]\n",
    "\n",
    "                    # Stack the fused and wrist data along the channel dimension\n",
    "                    fused_combined = np.concatenate((fused_data, wrist_acc, wrist_gyro), axis=-1)\n",
    "\n",
    "                    # Save the combined fused data\n",
    "                    output_subject_path = os.path.join(output_dir, relative_path)\n",
    "                    os.makedirs(output_subject_path, exist_ok=True)\n",
    "                    output_file_path = os.path.join(output_subject_path, file)\n",
    "                    np.save(output_file_path, fused_combined)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file}: {e}\")\n",
    "                    skipped += 1\n",
    "\n",
    "    print(f\"Skipped {skipped} files during fusion.\")\n",
    "\n",
    "# Combine data for each trial\n",
    "for trial in [\"trial1\", \"trial2\", \"trial3\"]:\n",
    "    fused_dir = fused_ankle_sd_paths[trial]\n",
    "    wrist_dir = wrist_rp_paths[trial]\n",
    "    output_dir = os.path.join(output_base_path, f\"{trial}_fused_images\")\n",
    "    combine_wrist_with_fused(fused_dir, wrist_dir, output_dir)\n",
    "\n",
    "print(\"Fusion completed. Combined dataset saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "is_project_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
